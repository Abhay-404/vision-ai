<h1>Vision-AI</h1>
Overview<br>
While sighted individuals can directly experience the worldâ€™s beauty, those with visual impairments often depend on others to narrate their environment. But what if their loved ones arenâ€™t present? To tackle this, weâ€™re creating a solution that acts as a perpetual companion for the visually impaired. Similar to a friend, our solution will verbally describe everything to them, and they can inquire about any aspect of the image. This ensures they never feel isolated or unsupported. This is the core of our project for the upcoming hackathon, where we aim to profoundly impact the lives of visually impaired individuals.

Features![visionai](https://github.com/Thejas775/vision-ai/assets/58774753/09bc0d3e-afd4-450e-9241-d692ed975988)

Audio Interaction ðŸ”Š : The project utilizes speech recognition and text-to-speech technologies to enable audio interaction with the system.

Firebase Integration: Firebase services are employed for tasks such as storing captured images, generating download URLs, and analyzing user feedback through Firebase analytics.

Llava Model /Gemini Pro vision ðŸ“ˆ: The project leverages the power of the Multimodal LLMs for processing images and providing relevant information.

Real-time Image Capture ðŸ“·: Vision-AI captures real-time images using the Raspberry Pi camera or mobile Camera module.<br>


How It Works<br>

<ol>
  <li>Audio Input: Users can interact with the system using voice commands such as "Hey Vision" or "Hello Vision."</li>

<li>Image Capture: Upon receiving the command, the system captures a real-time image using the Raspberry Pi camera or mobile Camera.</li>

<li>Prompt Recording: The user ask questions related to the captured image.</li>

<li>Firebase Integration: The captured image is uploaded to Firebase storage, and a download URL is generated.</li>

<li>Llava Model/Gemini Pro vision Processing ðŸ“ˆ: The multimodal LLM model is invoked with the image URL .</li>

<li>Text-to-Speech Output: The results generated by the Llava model/Gemini Pro vision are transformed into spoken words using text-to-speech technology, which are then relayed to the user.</li>

<li>Follw up question: The same cycles repeates.</li>

<li>Google Assistant:We will integrate the Google Assistant API with our model to enable users to access functionalities such as calling, chatting, and map navigation.
 </li>

</ol>
<h3>Demo Video:</h3>

[![Watch the video](https://img.youtube.com/vi/ot4cCV3151w/maxresdefault.jpg)](https://www.youtube.com/watch?v=ot4cCV3151w)<br>
<br>

For using open sourced LLava model we have used [Replicate](https://replicate.com/yorickvp/llava-13b).
export REPLICATE_API_TOKEN= <REPLICATE_API_TOKEN> <br>
run the above command in command prompt to activate the token

In the hackthon we will use Gemini pro-vision which is free for now.
